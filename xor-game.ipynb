{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# XOR game","metadata":{}},{"cell_type":"markdown","source":"The XOR function takes two binary digits as input and outputs, \n$$ \\mathrm{XOR}:\\{0,1\\}^2 \\to \\{0,1\\}$$\nand is defined by\n$$\\mathrm{XOR}(0,0) = 0, \\,\n\\mathrm{XOR}(1,0) = 1, \\\\\n\\mathrm{XOR}(0,1) = 1, \\, \\mathrm{XOR}(1,1) = 0. $$\n\nIt is known that there exists a neural network $f: \\mathbb{R}^2 \\to \\mathbb{R}$ which exactly fits the XOR function [GBC, pg. 174]. On the other hand, there does not exists a linear function from $\\mathbb{R}^2$ to $\\mathbb{R}$ exactly fitting XOR. \n\nThe widget below demonstates this. Run the full 'Code' cell at the bottom of the notebook, followed by the cell below.\n\n**Aim:** Using the sliders, find the values of the weights and biases of the neural network which exactly fit the XOR function. \n\nThe widget shows the output of the neural network (left), linear output layer (middle), as well as a discontinuous function which trivially fits XOR (right).\n\n**Neural network specifications:**\n- One hidden layer of width 2 with ReLU activation function. \n    - For this layer, the matrix of weights is denoted by $W \\in \\mathbb{R}^{2 \\times 2}$ and vector of biases is denoted by $c \\in \\mathbb{R}^{2}$.\n- Linear output layer.\n    - For this layer, the weights are denoted by $w \\in \\mathbb{R}^{2}$ and the bias is denoted by $b \\in \\mathbb{R}$.\n    \n[GBC] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\n","metadata":{}},{"cell_type":"code","source":"# Run this cell first - widget is below\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom __future__ import print_function\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nfrom IPython.display import display, Latex\n\ndef lin(x,w,b):\n    return np.dot(w,x) + b\n\ndef relu(t):\n    return np.maximum(0,t)\n\ndef nn1_reg(x,w1,w2,b1,b2):\n    '''\n    NN with 1 hidden layer (2 ReLu activation functions) and linear output unit\n    Layer 1 weights: w1 (2x2 array)\n    Layer 1 bias: b1 (1x2 array) \n    Layer 2 weights: w2 (1x2 array)\n    Layer 2 bias: b2 (float) \n    '''\n    x = np.array(x)\n    #layer1\n    h = relu(np.matmul(w1,x) + b1)\n    return  np.dot(w2,h) + b2\n\ndef XOR(x1,x2): # Takes floats between -0.5 and 1.49, rounding to nearest non-neg integer\n    x1, x2 = np.round(x1), np.round(x2)\n    if x1 + x2 == 2: return 0\n    else: return x1 + x2\nXOR = np.vectorize(XOR)\n\nw1_11 = widgets.FloatSlider(value=1, min=-2, max=2, step=0.01, description='$W_{11}$', orientation='vertical',continuous_update=False)\nw1_12 = widgets.FloatSlider(value=0, min=-2, max=2, step=0.01,description='$W_{12}$',orientation='vertical',continuous_update=False)\nw1_21= widgets.FloatSlider(value=0, min=-2, max=2, step=0.01,description='$W_{21}$',orientation='vertical',continuous_update=False)\nw1_22 = widgets.FloatSlider(value=1, min=-2, max=2, step=0.01,description='$W_{22}$',orientation='vertical',continuous_update=False)\nw2_1 = widgets.FloatSlider(value=1, min=-2, max=2, step=0.01,description='$w_{1}$',orientation='vertical',continuous_update=False)\nw2_2 = widgets.FloatSlider(value=-1, min=-2, max=2, step=0.01,description='$w_{2}$',orientation='vertical',continuous_update=False)\nb1_1 = widgets.FloatSlider(value=0, min=-2, max=2, step=0.01,description='$c_1$',orientation='vertical',continuous_update=False)\nb1_2 = widgets.FloatSlider(value=0, min=-2, max=2, step=0.01,description='$c_2$',orientation='vertical',continuous_update=False)\nb2 =  widgets.FloatSlider(value=0, min=-2, max=2, step=0.01,description='$b$',orientation='vertical')\nbutton = widgets.ToggleButton(value=False, description='Solution', disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    tooltip='Description',\n    icon='check' # (FontAwesome names without the `fa-` prefix)\n)\nui = widgets.HBox([w1_11, w1_12, w1_21, w1_22,b1_1,b1_2,w2_1,w2_2,b2,button])\ndef f(w1_11, w1_12, w1_21, w1_22,w2_1,w2_2,b1_1,b1_2,b2,button):\n    # Putting parameters into arrays\n    w1 = np.array([[w1_11,w1_12], [w1_21,w1_22]]) \n    w2 = np.array([w2_1,w2_2])\n    b1 = np.array([b1_1,b1_2])\n    \n    # Errors \n    nn_err = np.zeros((2,2))\n    lin_err = np.zeros((2,2))\n    for i in range(2):\n        for j in range(2):\n            nn_err[i,j] = round(abs(nn1_reg(np.array([i,j]),w1,w2,b1,b2) - XOR(i,j)),3)\n            lin_err[i,j] = round(abs(lin(np.array([i,j]),w2,b2) - XOR(i,j)),3)\n    print('Neural Network Error: Bit (0,0):', nn_err[0,0], ', Bit (0,1):', nn_err[0,1], ', Bit (1,0):', nn_err[1,0], ', Bit (1,1):', nn_err[1,1])\n    print('Linear Error:         Bit (0,0):', lin_err[0,0], ', Bit (0,1):', lin_err[0,1], ', Bit (1,0):', lin_err[1,0], ', Bit (1,1):', lin_err[1,1])\n    if button == True:\n        display(Latex('Solution: $\\qquad \\qquad \\qquad   W_{1 1} = 1, \\, W_{1 2} = 1, \\,W_{2 1} = 1, \\,W_{2 2} = 1, \\,c_{1} = 0, \\,c_{2} = -1, \\, w_{1} = 1, \\,w_{2} = -2, \\,b = 0$'))\n        \n    # Plotting\n    \n    fig, axs = plt.subplots(1,3,figsize=(15,15))\n    x1_lst = np.linspace(-0.2,1.2,100)\n    x2_lst = np.linspace(-0.2,1.2,100)\n    x_grid = np.meshgrid(x1_lst,x2_lst)\n    \n    nn_grid = np.zeros([100,100])\n    for i in range(100):\n        for j in range(100):\n            nn_grid[i,j] = nn1_reg([x_grid[0][i,j],x_grid[1][i,j]],w1,w2,b1,b2)\n            \n    lin_grid = np.zeros([100,100])\n    for i in range(100):\n        for j in range(100):\n            lin_grid[i,j] = lin(np.array([x_grid[0][i,j],x_grid[1][i,j]]),w2,b2)\n    \n    xor_grid = XOR(x_grid[0],x_grid[1])\n    \n    \n    # Left axis (Neural Network)\n    im1 = axs[0].imshow(nn_grid, extent=[x1_lst[0],x1_lst[-1],x2_lst[0],x2_lst[-1]])\n    axs[0].title.set_text('Neural Network')\n    axs[0].scatter([0,1],[0,1],marker = '$1$',color = 'white')\n    axs[0].scatter([1,0],[0,1],marker = '$0$',color = 'white')\n    plt.colorbar(im1, ax=axs[0], fraction=0.046, pad=0.04) # 'fraction' and 'pad' parameters scale the colorbar to the size of the graph \n    \n    # Middle axis (Linear)\n    im2  = axs[1].imshow(lin_grid,extent=[x1_lst[0],x1_lst[-1],x2_lst[0],x2_lst[-1]])\n    axs[1].title.set_text('Linear')\n    axs[1].scatter([0,1],[0,1],marker = '$1$',color = 'white')\n    axs[1].scatter([1,0],[0,1],marker = '$0$',color = 'white')\n    plt.colorbar(im2, ax=axs[1], fraction=0.046, pad=0.04)\n    \n    # Right axis (Discontinuous)\n    im3 = axs[2].imshow(xor_grid,extent=[x1_lst[0],x1_lst[-1],x2_lst[0],x2_lst[-1]])\n    axs[2].title.set_text('Discontinuous solution')\n    axs[2].scatter([0,1],[0,1],marker = '$1$',color = 'white')\n    axs[2].scatter([1,0],[0,1],marker = '$0$',color = 'white')\n    plt.colorbar(im3, ax=axs[2], fraction=0.046, pad=0.04)\n    \n    fig.tight_layout()\n    \n    \n        \nout = widgets.interactive_output(f, {'w1_11': w1_11, 'w1_12': w1_12, 'w1_21': w1_21, 'w1_22': w1_22,\n                                    'w2_1': w2_1, 'w2_2': w2_2, 'b1_1': b1_1, 'b1_2': b1_2, 'b2' : b2, 'button': button})\n","metadata":{"execution":{"iopub.status.busy":"2023-08-04T15:37:56.325953Z","iopub.execute_input":"2023-08-04T15:37:56.326617Z","iopub.status.idle":"2023-08-04T15:37:58.593220Z","shell.execute_reply.started":"2023-08-04T15:37:56.326585Z","shell.execute_reply":"2023-08-04T15:37:58.591834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run this after running the above code cell.\ndisplay(ui, out)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T15:37:58.595825Z","iopub.execute_input":"2023-08-04T15:37:58.596300Z","iopub.status.idle":"2023-08-04T15:37:58.610044Z","shell.execute_reply.started":"2023-08-04T15:37:58.596248Z","shell.execute_reply":"2023-08-04T15:37:58.608864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}