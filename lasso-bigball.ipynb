{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Convergence to endpoints with high probability for LASSO ","metadata":{}},{"cell_type":"markdown","source":"#### Alexei Stepanenko","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom scipy.stats import uniform_direction, uniform, bernoulli\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-10-31T19:29:46.553096Z","iopub.execute_input":"2023-10-31T19:29:46.553738Z","iopub.status.idle":"2023-10-31T19:29:50.583941Z","shell.execute_reply.started":"2023-10-31T19:29:46.553691Z","shell.execute_reply":"2023-10-31T19:29:50.583151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Consider the unconstrained LASSO problem\n$$\n\\textrm{argmin}_{w\\in \\mathbb{R}^d} \\|X w - y\\|_2^2 + \\|w\\|_1\n$$\nwhere $y \\in \\mathbb{R}^m$ and $X \\in \\mathbb{R}^{m \\times d}$ are defined by $y = e_1$ and \n$$\nX = (\\alpha \\quad 1 \\quad \\cdots \\quad  1  ) \\,\\oplus \\, (I_{m-1} \\quad 0_{d - m - 1})\n$$\nfor some $\\alpha > 0$, where there are $N$ ones. \n\nThe solution always satisfies $w_3 = \\cdots = w_d = 0$ so we can consider for simplicity the problem\n$$\n\\textrm{argmin}_{w_1, w_2\\in \\mathbb{R}} |\\alpha w_1 + w_2 + \\cdots + w_N - 1|^2 + \\alpha |w_1| + \\cdots + |w_N|.\n$$\nWe focus on the case $\\alpha = 1$, in which case the solution is the convex hull of $\\{\\tfrac{1}{2} e_1, ...,\\tfrac{1}{2} e_N\\}$.  Furthermore, the solution can become either one of endpoints (shifted slightly) with an arbitrarily small perturbation of $\\alpha$ away from $1$.\n\n**Experiment:** Fix $\\epsilon\\geq0$. Run the ISTA algorithm until convergence (until the loss is less than $ 3/4 + \\epsilon$), with $(w_1,..., w_N)$ initialised according to a uniform distribution in a $N-1$ dimensional sphere  $\\partial B_R(0)$.","metadata":{}},{"cell_type":"code","source":"def shrink_op(x,lr):\n    'Shrinkage operator for ISTA'\n    return F.relu(abs(x) - lr)*torch.sign(x)\n\ndef train_ISTA_conv(w, alpha = 1, ep = 10**(-4), lr = 0.1, max_steps = 10000000):\n    '''\n    Returns ISTA training paths for initial condition `w` (one dimensional torch tensor)\n    See https://www.ceremade.dauphine.fr/~carlier/FISTA , eq. 1.4 and 1.5.\n    `ep`: tolerance. `lr`: learning rate. `max_steps`: max number of steps\n    '''\n    w = w.clone().detach().requires_grad_(True)\n    loss_hist = []\n    w_hist = [w.detach().numpy()]  # list of torch tensors\n    N = len(w)\n    X = torch.eye(N)\n    X[0,0] = alpha\n    Xw = X @ w\n    n = 0\n    loss = (Xw.sum() - 1)**2 + abs(Xw).sum()\n    while loss.item() >= 3/4 + ep and n <= max_steps:\n        Xw = X @ w\n        loss_noreg = (Xw.sum() - 1)**2 \n        loss = loss_noreg + abs(Xw).sum()\n        loss_hist.append(loss.item())\n        loss_noreg.backward()\n        w.data = shrink_op(w.data - lr * w.grad.data, lr)\n        w.grad.data.zero_()\n        w_hist.append(w.detach().numpy())\n        n += 1\n    if n > max_steps:\n        print('TERMINATED DUE TO RUNTIME')\n    return np.array(w_hist), loss_hist\n\ndef distCorner(w, N):\n    \"\\ell^2 distance of w (nd array) to convex hull of {0.5*e_1,...,0.5*e_N}\"\n    dists_to_corners = []\n    for i in range(N):\n        corner  = np.zeros(N)\n        corner[i] = 0.5\n        dists_to_corners.append(np.linalg.norm(corner - w))\n    return min(dists_to_corners)\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-31T19:29:50.585486Z","iopub.execute_input":"2023-10-31T19:29:50.585918Z","iopub.status.idle":"2023-10-31T19:29:50.596606Z","shell.execute_reply.started":"2023-10-31T19:29:50.585892Z","shell.execute_reply":"2023-10-31T19:29:50.594856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Testing: reproducing past results","metadata":{}},{"cell_type":"code","source":"w_hist, loss_hist = train_ISTA_conv(torch.tensor([2,1], dtype = torch.float32))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w_hist = w_hist.transpose()\nplt.plot(w_hist[0],w_hist[1])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(loss_hist)\nplt.plot([0,len(loss_hist)],[3/4,3/4])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"R = 2; num_samp = 1000; tol = 0.01\n\nw0_samps = R*uniform_direction.rvs(dim=2, size=num_samp)\nw_final_samps = []\nfor w0 in w0_samps:\n    w_final, train_hist = train_ISTA_conv(torch.tensor(w0, dtype = torch.float32))\n    w_final = w_final[-1]\n    w_final_samps.append(w_final)\ndistEnd_samps = [distCorner(w, 2) for w in w_final_samps]\nmean_distEnd = np.array(distEnd_samps).mean()\nstd_distEnd = np.array(distEnd_samps).std()\n\npos_lst = []\nneg_lst = []\nfor i in range(len(w0_samps)):\n    if distEnd_samps[i] < tol:\n        pos_lst.append(np.array(w0_samps[i]))\n    else:\n        neg_lst.append(np.array(w0_samps[i]))\npos_lst = np.array(pos_lst).transpose()\nneg_lst = np.array(neg_lst).transpose()\nplt.figure(figsize = (5,5))\nplt.scatter(pos_lst[0], pos_lst[1], label= 'Converge to endpoint')\nplt.scatter(neg_lst[0], neg_lst[1], label= 'Dont converge to endpoint')\nplt.plot([1/2,0],[0,1/2], color = 'black', lw = 2)\nplt.xlabel(r'$w_1$')\nplt.ylabel(r'$w_2$')\nplt.legend(loc = 1)\nplt.show()\n\nprint('Mean distance to endpoint', mean_distEnd)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"R = 20; num_samp = 1000; tol = 0.01\n\nw0_samps = R*uniform_direction.rvs(dim=2, size=num_samp)\nw_final_samps = []\nfor w0 in w0_samps:\n    w_final, train_hist = train_ISTA_conv(torch.tensor(w0, dtype = torch.float32))\n    w_final = w_final[-1]\n    w_final_samps.append(w_final)\ndistEnd_samps = [distCorner(w, 2) for w in w_final_samps]\nmean_distEnd = np.array(distEnd_samps).mean()\nstd_distEnd = np.array(distEnd_samps).std()\n\npos_lst = []\nneg_lst = []\nfor i in range(len(w0_samps)):\n    if distEnd_samps[i] < tol:\n        pos_lst.append(np.array(w0_samps[i]))\n    else:\n        neg_lst.append(np.array(w0_samps[i]))\npos_lst = np.array(pos_lst).transpose()\nneg_lst = np.array(neg_lst).transpose()\nplt.figure(figsize=(5,5))\nplt.scatter(pos_lst[0], pos_lst[1], label= 'Converge to endpoint')\nplt.scatter(neg_lst[0], neg_lst[1], label= 'Dont converge to endpoint')\nplt.plot([1/2,0],[0,1/2], color = 'black', lw = 2)\nplt.xlabel(r'$w_1$')\nplt.ylabel(r'$w_2$')\nplt.legend(loc = 1)\nplt.show()\n\nprint('Mean distance to endpoint', mean_distEnd)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"R = 2; num_samp = 3000; alpha = 1\n\nw0_samps = R*uniform_direction.rvs(dim=3, size=num_samp)\nw_final_samps = []\nfor w0 in w0_samps:\n    w_final, train_hist = train_ISTA_conv(torch.tensor(w0, dtype = torch.float32))\n    w_final = w_final[-1]\n    w_final_samps.append(w_final)\ndistEnd_samps = [distCorner(w, 3) for w in w_final_samps]\nmean_distEnd = np.array(distEnd_samps).mean()\nstd_distEnd = np.array(distEnd_samps).std()\n\ntol = 0.01\npos1_lst = []\npos2_lst = []\npos3_lst = []\nneg_lst = []\nfor i in range(len(w0_samps)):\n    if distEnd_samps[i] < tol:\n        if abs(w_final_samps[i][0] - 1/2) < tol:\n            pos1_lst.append(np.array(w0_samps[i]))\n        if abs(w_final_samps[i][1] - 1/2) < tol:\n            pos2_lst.append(np.array(w0_samps[i]))\n        if abs(w_final_samps[i][2] - 1/2) < tol:\n            pos3_lst.append(np.array(w0_samps[i]))\n    else:\n        neg_lst.append(np.array(w0_samps[i]))\npos1_lst = np.array(pos1_lst).transpose()\npos2_lst = np.array(pos2_lst).transpose()\npos3_lst = np.array(pos3_lst).transpose()\nneg_lst = np.array(neg_lst).transpose()\nfig = plt.figure(figsize = (5,5))\nax = fig.add_subplot(projection='3d')\nax.scatter(pos1_lst[0], pos1_lst[1], pos1_lst[2], label= 'Converge to endpoint 1')\nax.scatter(pos2_lst[0], pos2_lst[1], pos2_lst[2], label= 'Converge to endpoint 2')\nax.scatter(pos3_lst[0], pos3_lst[1], pos3_lst[2], label= 'Converge to endpoint 3')\nax.scatter(neg_lst[0], neg_lst[1], neg_lst[2], label= 'Dont converge to endpoint')\nplt.xlabel(r'$w_1$')\nplt.ylabel(r'$w_2$')\nplt.ylabel(r'$w_3$')\nplt.legend(loc = 1)\nplt.show()\n\nprint('Mean distance to endpoint', mean_distEnd)\nprint('Proportion converging to e_1', len(pos1_lst[0])/len(w0_samps))\nprint('Proportion converging to e_2', len(pos2_lst[0])/len(w0_samps))\nprint('Proportion converging to e_3', len(pos3_lst[0])/len(w0_samps))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"R = 10; num_samp = 3000; alpha = 1\n\nw0_samps = R*uniform_direction.rvs(dim=3, size=num_samp)\nw_final_samps = []\nfor w0 in w0_samps:\n    w_final, train_hist = train_ISTA_conv(torch.tensor(w0, dtype = torch.float32))\n    w_final = w_final[-1]\n    w_final_samps.append(w_final)\ndistEnd_samps = [distCorner(w, 3) for w in w_final_samps]\nmean_distEnd = np.array(distEnd_samps).mean()\nstd_distEnd = np.array(distEnd_samps).std()\n\ntol = 0.01\npos1_lst = []\npos2_lst = []\npos3_lst = []\nneg_lst = []\nfor i in range(len(w0_samps)):\n    if distEnd_samps[i] < tol:\n        if abs(w_final_samps[i][0] - 1/2) < tol:\n            pos1_lst.append(np.array(w0_samps[i]))\n        if abs(w_final_samps[i][1] - 1/2) < tol:\n            pos2_lst.append(np.array(w0_samps[i]))\n        if abs(w_final_samps[i][2] - 1/2) < tol:\n            pos3_lst.append(np.array(w0_samps[i]))\n    else:\n        neg_lst.append(np.array(w0_samps[i]))\npos1_lst = np.array(pos1_lst).transpose()\npos2_lst = np.array(pos2_lst).transpose()\npos3_lst = np.array(pos3_lst).transpose()\nneg_lst = np.array(neg_lst).transpose()\nfig = plt.figure(figsize = (5,5))\nax = fig.add_subplot(projection='3d')\nax.scatter(pos1_lst[0], pos1_lst[1], pos1_lst[2], label= 'Converge to endpoint 1')\nax.scatter(pos2_lst[0], pos2_lst[1], pos2_lst[2], label= 'Converge to endpoint 2')\nax.scatter(pos3_lst[0], pos3_lst[1], pos3_lst[2], label= 'Converge to endpoint 3')\nax.scatter(neg_lst[0], neg_lst[1], neg_lst[2], label= 'Dont converge to endpoint')\nplt.xlabel(r'$w_1$')\nplt.ylabel(r'$w_2$')\nplt.ylabel(r'$w_3$')\nplt.legend(loc = 1)\nplt.show()\n\nprint('Mean distance to endpoint', mean_distEnd)\nprint('Proportion converging to e_1', len(pos1_lst[0])/len(w0_samps))\nprint('Proportion converging to e_2', len(pos2_lst[0])/len(w0_samps))\nprint('Proportion converging to e_3', len(pos3_lst[0])/len(w0_samps))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Rate of conv. as $R \\to \\infty$ for varying $N$ ","metadata":{}},{"cell_type":"markdown","source":"**To do:** Add error bars by repeating experiment multiple times and computing mean/std","metadata":{}},{"cell_type":"code","source":"def give_probs_sphere(R, num_samp, alpha, N, tol = 0.01):\n    'Gives probability convergence to endpoint and point e_1 upon random initialisation on sphere of radius `R`'\n    w0_samps = R*uniform_direction.rvs(dim=N, size=num_samp)\n    w_final_samps = []\n    for w0 in w0_samps:\n        w_final, train_hist = train_ISTA_conv(torch.tensor(w0, dtype = torch.float32), alpha = alpha)\n        w_final = w_final[-1]\n        w_final_samps.append(w_final)\n    distEnd_samps = [distCorner(w, N) for w in w_final_samps]\n    e1 = np.zeros(N); e1[0] = 1\n    dist_e1_samps = [np.linalg.norm(w - 0.5*e1) for w in w_final_samps]\n    num_1 = 0\n    num_pos = 0\n    for i in range(num_samp):\n        if distEnd_samps[i] < tol:\n            num_pos += 1\n            if dist_e1_samps[i] < tol:\n                num_1 += 1\n    prob_end = num_pos/num_samp\n    prob_1 = num_1/num_samp\n    return prob_end, prob_1","metadata":{"execution":{"iopub.status.busy":"2023-10-31T19:29:50.598292Z","iopub.execute_input":"2023-10-31T19:29:50.598672Z","iopub.status.idle":"2023-10-31T19:29:50.612966Z","shell.execute_reply.started":"2023-10-31T19:29:50.598643Z","shell.execute_reply":"2023-10-31T19:29:50.612056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We look at final distance to an endpoint and to $e_1$ as $R \\to \\infty$ for varying $N$ (fix $\\alpha =1$)","metadata":{}},{"cell_type":"code","source":"num_samp  = 5000\nR_vals = np.linspace(3,20,12)\nN_vals =[2,3,5,8]\nalpha = 1\n\nend_vals_all = []; e1_vals_all = []\nfor N in N_vals:\n    end_vals = []; e1_vals = []\n    for R in tqdm(R_vals):\n        prob_end, prob_1 = give_probs_sphere(R, num_samp, alpha, N)\n        end_vals.append(prob_end); e1_vals.append(prob_1)\n    end_vals_all.append(end_vals); e1_vals_all.append(e1_vals)\n\nfig, ax = plt.subplots(1,2, figsize= (24,10))\nfor i in range(len(N_vals)):\n    label = 'N = ' + str(N_vals[i])\n    ax[0].plot(R_vals,end_vals_all[i], label = label)\nax[0].set_ylabel('Probability of convergence to endpoint')\nax[0].set_xlabel('R')\nax[0].legend()\nax[0].plot([R_vals[0],R_vals[-1]],[1,1],linestyle ='--', color = 'gray')\nfor i in range(len(N_vals)):\n    label = 'N = ' + str(N_vals[i])\n    ax[1].plot(R_vals,e1_vals_all[i], label = label)\nax[1].set_ylabel('Probability of convergence to $e_1$')\nax[1].set_xlabel('R')\nax[1].legend()\nfor N in N_vals:\n    ax[1].plot([R_vals[0],R_vals[-1]],[1/N,1/N],linestyle ='--', color = 'gray')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T19:43:20.686417Z","iopub.execute_input":"2023-10-31T19:43:20.686778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We look at final distance to an endpoint and to $e_1$ as $R \\to \\infty$ for varying $\\alpha$ (fix $N = 2$)","metadata":{}},{"cell_type":"code","source":"num_samp  = 5000\nR_vals = np.linspace(3,40,12)\nN = 2\nal_vals = [0.9,0.99, 0.999, 1, 1.001, 1.01]\n\nend_vals_all = []; e1_vals_all = []\nfor alpha in al_vals:\n    end_vals = []; e1_vals = []\n    for R in tqdm(R_vals):\n        prob_end, prob_1 = give_probs_sphere(R, num_samp, alpha, N)\n        end_vals.append(prob_end); e1_vals.append(prob_1)\n    end_vals_all.append(end_vals); e1_vals_all.append(e1_vals)\n\nfig, ax = plt.subplots(1,2, figsize= (24,10))\nfor i in range(len(al_vals)):\n    label = 'alpha = ' + str(al_vals[i])\n    ax[0].plot(R_vals,end_vals_all[i], label = label)\nax[0].set_ylabel('Probability of convergence to endpoint')\nax[0].set_xlabel('R')\nax[0].legend()\nax[0].plot([R_vals[0],R_vals[-1]],[1,1],linestyle ='--', color = 'gray')\nfor i in range(len(al_vals)):\n    label = 'alpha = ' + str(al_vals[i])\n    ax[1].plot(R_vals,e1_vals_all[i], label = label)\nax[1].set_ylabel('Probability of convergence to $e_1$')\nax[1].set_xlabel('R')\nax[1].legend()\nax[1].plot([R_vals[0],R_vals[-1]],[0,0],linestyle ='--', color = 'gray')\nax[1].plot([R_vals[0],R_vals[-1]],[1/2,1/2],linestyle ='--', color = 'gray')\nax[1].plot([R_vals[0],R_vals[-1]],[1,1],linestyle ='--', color = 'gray')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T19:42:31.349817Z","iopub.status.idle":"2023-10-31T19:42:31.350131Z","shell.execute_reply.started":"2023-10-31T19:42:31.349983Z","shell.execute_reply":"2023-10-31T19:42:31.349999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I don't run $\\alpha = 1.1$ as it takes way too long to converge for some reason.  ","metadata":{}},{"cell_type":"markdown","source":"### Bonus: Plotting trajectories","metadata":{}},{"cell_type":"code","source":"R = 5; tol = 0.01\n\nw0_samps = R*np.array([[np.cos(th), np.sin(th)] for th in np.linspace(0,2*np.pi,1000)])\nw_final_samps = []\ntrainhist_samps = []\nfor w0 in w0_samps:\n    w_final, train_hist = train_ISTA_conv(torch.tensor(w0, dtype = torch.float32))\n    trainhist_samps.append(w_final)\n    w_final = w_final[-1]\n    w_final_samps.append(w_final)\ndistEnd_samps = [distCorner(w, 2) for w in w_final_samps]\n\npos_lst = []; neg_lst = []\nposhist_lst = []; neghist_lst = []\nfor i in range(len(w0_samps)):\n    if distEnd_samps[i] < tol:\n        pos_lst.append(np.array(w0_samps[i]))\n        poshist_lst.append(trainhist_samps[i])\n    else:\n        neg_lst.append(np.array(w0_samps[i]))\n        neghist_lst.append(trainhist_samps[i])\npos_lst = np.array(pos_lst).transpose()\nneg_lst = np.array(neg_lst).transpose()\nplt.figure(figsize = (11,11))\nplt.scatter(pos_lst[0], pos_lst[1], label= 'Converge to endpoint', color = 'green' )\nplt.scatter(neg_lst[0], neg_lst[1], label= 'Dont converge to endpoint', color = 'red')\nfor traj in poshist_lst:\n    traj = np.array(traj).transpose()\n    plt.plot(traj[0], traj[1], color = 'green', alpha = 0.1)\nfor traj in neghist_lst:\n    traj = np.array(traj).transpose()\n    plt.plot(traj[0], traj[1], color = 'red', alpha = 0.1)\nplt.plot([1/2,0],[0,1/2], color = 'black', lw = 2)\nplt.xlabel(r'$w_1$')\nplt.ylabel(r'$w_2$')\nplt.legend(loc = 1)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}